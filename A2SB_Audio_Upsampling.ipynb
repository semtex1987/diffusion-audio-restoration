{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2SB Audio Upsampling\n",
    "\n",
    "This notebook allows you to upscale an audio file using the A2SB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Clone Repository and Install Dependencies\n",
    "!git clone https://github.com/NVIDIA/diffusion-audio-restoration.git\n",
    "%cd diffusion-audio-restoration\n",
    "!pip install -q numpy scipy matplotlib jsonargparse librosa soundfile torch torchaudio einops pytorch_lightning rotary_embedding_torch ssr_eval ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "from google.colab import files\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from subprocess import Popen, PIPE\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "def load_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "def save_yaml(data, prefix=\"./temp_config\"):\n",
    "    os.makedirs(os.path.dirname(prefix), exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rnd_num = np.random.rand()\n",
    "    rnd_num = rnd_num - rnd_num % 0.000001\n",
    "    file_name = f\"{prefix}_{timestamp}_{rnd_num}.yaml\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        yaml.dump(data, f)\n",
    "    return file_name\n",
    "\n",
    "def shell_run_cmd(cmd):\n",
    "    print('running:', cmd)\n",
    "    p = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "    stdout, stderr = p.communicate()\n",
    "    print(stdout.decode())\n",
    "    print(stderr.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upload Audio File\n",
    "Run the following cell to upload your audio file. Make sure it's in a format that `librosa` can read (e.g., .wav, .mp3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f784260d-619f-4204-f597-9e9f743c3f71"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving test.wav to test.wav\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "uploaded = files.upload()\n",
    "input_filename = list(uploaded.keys())[0]\n",
    "print(f'Uploaded file: {input_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_URL = \"https://huggingface.co/nvidia/audio_to_audio_schrodinger_bridge/resolve/main/ckpt/A2SB_onesplit_0.0_1.0_release.ckpt?download=true\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "CHECKPOINT_FILENAME = \"A2SB_onesplit_0.0_1.0_release.ckpt\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    print(\"Downloading checkpoint...\")\n",
    "    shell_run_cmd(f\"wget -O {CHECKPOINT_PATH} {CHECKPOINT_URL}\")\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Checkpoint already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolloff_freq(audio_file, roll_percent=0.99):\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=roll_percent)[0]\n",
    "    rolloff = int(np.mean(rolloff))\n",
    "    print('99 percent rolloff:', rolloff)\n",
    "    return rolloff\n",
    "\n",
    "def upsample_one_sample(audio_filename, output_audio_filename, predict_n_steps=50):\n",
    "    assert output_audio_filename != audio_filename, \"output filename cannot be input filename\"\n",
    "\n",
    "    inference_config = load_yaml('configs/inference_files_upsampling.yaml')\n",
    "    inference_config['data']['predict_filelist'] = [\n",
    "        {\n",
    "            'filepath': f'../{audio_filename}',\n",
    "            'output_subdir': '.'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    cutoff_freq = compute_rolloff_freq(f'../{audio_filename}', roll_percent=0.99)\n",
    "    inference_config['data']['transforms_aug'][0]['init_args']['upsample_mask_kwargs'] = {\n",
    "        'min_cutoff_freq': cutoff_freq,\n",
    "        'max_cutoff_freq': cutoff_freq\n",
    "    }\n",
    "    temporary_yaml_file = save_yaml(inference_config)\n",
    "\n",
    "    cmd = (\n",
    "        f\"python ensembled_inference_api.py predict \"\n",
    "        f\"-c configs/onesplit.yaml \"\n",
    "        f\"-c {temporary_yaml_file} \"\n",
    "        f\"--model.predict_n_steps={predict_n_steps} \"\n",
    "        f\"--model.output_audio_filename=../{output_audio_filename}\"\n",
    "    )\n",
    "    shell_run_cmd(cmd)\n",
    "    \n",
    "    os.remove(temporary_yaml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Upsampling\n",
    "Run the following cell to start the upsampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f\"restored_{input_filename}\"\n",
    "upsample_one_sample(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Audio:\")\n",
    "display(Audio(input_filename))\n",
    "\n",
    "print(\"Upscaled Audio:\")\n",
    "display(Audio(output_filename))\n",
    "\n",
    "print(\"Download the upscaled audio:\")\n",
    "files.download(output_filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
